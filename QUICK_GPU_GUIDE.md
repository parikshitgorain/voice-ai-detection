# Quick GPU Setup Reference

## What to Choose from Your Cloud Provider Image/Dashboard

### ğŸ¯ RECOMMENDED GPU (Best Value)
**NVIDIA Tesla T4**
- Cost: ~$0.30-0.50/hour
- Memory: 16GB
- Perfect for AI inference
- 5-10x faster than CPU

### ğŸ” What to Look For in the Image/Menu:

| Look For | Select |
|----------|--------|
| **Instance Type** | "GPU", "Accelerated", "GPU-Optimized" |
| **GPU Model** | T4, A10, V100, or A100 |
| **Driver** | "NVIDIA", "CUDA-enabled" |
| **OS** | Ubuntu 20.04+ (you have this âœ“) |

### âŒ Avoid These:
- "CPU optimized" (no GPU)
- "Memory optimized" (no GPU)
- "Graphics" without NVIDIA (might be AMD/integrated)

---

## ğŸš€ After Creating GPU Instance

### Step 1: Verify GPU
```bash
nvidia-smi
```
Should show your GPU details.

### Step 2: Run Setup Script
```bash
cd /var/www/voice-ai-detection
./scripts/setup_gpu.sh
```

### Step 3: Restart Server
```bash
pm2 restart voice-ai-detection
# OR
sudo systemctl restart voice-ai-detection
```

### Step 4: Verify Usage
```bash
# Check what device is being used:
node -e "console.log(require('./backend/utils/gpu_helper').getDevice())"
# Should show: cuda

# Monitor GPU during API calls:
watch -n 1 nvidia-smi
```

---

## ğŸ’¡ How It Works (Removable GPU)

### Current System Behavior:
```
Your VPS â†’ Check GPU â†’ Found? â†’ Use GPU (fast)
                     â†’ Not Found? â†’ Use CPU (slower but works)
```

### You Can:
1. âœ… Start with CPU â†’ Add GPU later â†’ System auto-uses GPU
2. âœ… Start with GPU â†’ Remove GPU â†’ System auto-falls back to CPU
3. âœ… Force CPU even with GPU available: `export DEEP_MODEL_DEVICE=cpu`
4. âœ… Force GPU: `export DEEP_MODEL_DEVICE=cuda`
5. âœ… Auto-detect (default): No environment variable needed

### No Reinstall Needed!
The system adapts automatically when you:
- Attach/detach GPU
- Switch instances
- Upgrade/downgrade server

---

## ğŸ“Š Current Status

**Your System Right Now:**
- Device: `CPU` (no GPU detected)
- Works: âœ… Yes
- Speed: Normal (baseline)

**If You Add GPU:**
- Device: `CUDA` (auto-detected)
- Works: âœ… Yes  
- Speed: 5-10x faster

---

## ğŸ® Cloud Provider Examples

### AWS EC2
1. Choose instance: `g4dn.xlarge` (has T4 GPU)
2. AMI: Ubuntu 20.04+
3. Instance has nvidia-smi pre-installed
4. Run setup script â†’ Done!

### Google Cloud
1. Machine type: n1-standard-4
2. âœ… Add: "1 x NVIDIA Tesla T4"
3. Install GPU drivers: `sudo /opt/google/cuda-installer/cuda-installer`
4. Run setup script â†’ Done!

### Azure
1. VM size: NC4as T4 v3
2. Has T4 GPU included
3. Run setup script â†’ Done!

### Hetzner Cloud  
1. Server type: "GPU-enabled"
2. Select: Nvidia GPU option
3. Run setup script â†’ Done!

---

## ğŸ†˜ Can't Find the Screenshot?

Send me details about:
1. What cloud provider? (AWS/GCP/Azure/other)
2. What page are you on? (Instance creation/VM settings)
3. What options do you see?

I'll tell you exactly what to select!

---

## ğŸ¯ Summary

**For Your PNG Image:**
- Look for **"GPU"** or **"NVIDIA"** keywords
- Choose **T4** if available (best value)
- If no T4, choose **A10** or **any NVIDIA GPU**
- Avoid non-GPU instances

**After Setup:**
- GPU = Removable extension âœ…
- Auto-falls back to CPU âœ…
- No code changes needed âœ…
- Works on any VPS âœ…

**Ready to use!** ğŸš€
